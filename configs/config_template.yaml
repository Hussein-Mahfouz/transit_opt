input:
  gtfs_path: # The initial gtfs feed that you start the optimisation with
      - data/external/study_area_gtfs_bus.zip
  allowed_headways: [10, 15, 30, 60, 120] # in minutes
  # We split the day into equal intervals. How long is each interval?
  interval_hours: 4
  # The PT and zoning layer are clipped to this boundary
  boundary_geojson: data/external/boundaries/study_area_boundary.geojson
  boundary_buffer_km: 2.0 # How much to buffer the boundary layer by (km)

  # drt config used to define drt zones and fleet size options in each zone
  drt:
    enabled: true
    target_crs: "EPSG:3857"  # Web Mercator for consistency
    default_drt_speed_kmh: 25.0  # Default speed for all DRT zones
    zones:
      - zone_id: drt_ne
        service_area_path: data/external/drt/drt_ne.shp # Service area shapefile for this DRT zone
        allowed_fleet_sizes: [0, 10, 25, 50, 100]  # Fleet size options for this zone
        zone_name: "Leeds NE DRT" # name is used in the saved .json file
        drt_speed_kmh: 20.0  # Zone-specific speed (Will use default_drt_speed_kmh if zone-specific speed is not provided)
      - zone_id: drt_nw
        service_area_path: data/external/drt/drt_nw.shp
        allowed_fleet_sizes: [0, 10, 25, 50, 100]  # Different fleet options
        zone_name: "Leeds NW DRT"

problem:
  objective:
    type: WaitingTimeObjective # StopCoverageObjective, WaitingTimeObjective
    spatial_resolution_km: 2.0 # The size of each zone in the grid
    crs: "EPSG:3857"
    # time_aggregation: How to aggregate values across time intervals
      # WaitingTimeObjective: [average, peak, sum, intervals]
          # average:  Average waiting time across intervals for each zone
          # sum: sum waiting times across intervals for each zone
          # peak: Use waiting time from interval with most vehicles per zone
          # intervals: Calculate objective for each interval, then average
      # StopCoverageObjective [average, peak, sum]
          # average:  Average vehicles across intervals for each zone
          # sum: sum vehicles across intervals for each zone
          # peak: Use vehicles per zone from interval with most vehicles per zone
    time_aggregation: average # WaitingTimeObjective: [average, peak, sum, intervals]. StopCoverageObjective [average, peak, sum]
    metric: variance # Only applies for WaitingTimeObjective. Options: [variance, total].

    # WEIGHTING (you can only use one of population_weighted and demand_weighted, not both)
    ##### Demand weighting configuration
    population_weighted: true # Weight objective values per zone by population or not
    # Tif file from worldpop. Used when population_weighted: true
    population_layer: data/external/population/gbr_pop_2025_CN_1km_R2025A_UA_v1.tif
    population_power: 1 # Population weighting exponent
    #####
    ##### Demand weighting configuration
    demand_weighted: true
    trip_data_path: data/external/trips/leeds_trips.csv
    trip_data_crs: "EPSG:3857"  # ← NEW: Explicitly specify CRS
    demand_power: 1.0
    min_trip_distance_m: 1000  # Optional: filter short trips
    #####
    
    spatial_lag: false  # No spatial lag
    # An alpha of 0 means no spatial lag, and an alpha of 1 means that the objective value for a zone is the average of the objective values of all neighboring zones.
    # Values between 0 and 1 are a weighted average of the zone's own value and the average of its neighbors.
    alpha: 0.0

  constraints:
    - type: FleetTotalConstraintHandler
      # baseline: How to determine baseline fleet
      #    - 'current_peak': Use peak fleet from current GTFS
      #    - 'current_average': Use average fleet from current GTFS
      #    - 'manual': Use manually specified value
      baseline: current_peak
      # if value = 0.35 then 35% increase allowed
      tolerance: 0.25
      # measure: Which measure to constrain
      #    - 'peak': Constrain peak interval fleet
      #    - 'average': Constrain average across intervals
      #    - 'total': Constrain sum across all intervals (default)
      measure: average
    - type: MinimumFleetConstraintHandler
      # baseline: Which original fleet measure to use as reference
      #    - 'current_peak': Reference original peak fleet
      #    - 'current_by_interval': Reference original per-interval fleet
      #    - 'current_average': Reference original average fleet
      baseline: current_average
      min_fleet_fraction: 0.95 # Maintain at least X% of current service
      # level: Constraint application level
      #    - 'system': Single constraint for entire system
      #    - 'interval': One constraint per time interval
      level: system
      # measure: What to measure when level='system'
      #    - 'peak': Constrain peak interval fleet
      #    - 'average': Constrain average across intervals
      #    - 'total': Constrain sum across all intervals
      measure: average
    - type: FleetPerIntervalConstraintHandler
      # baseline (str): How to determine baseline fleet per interval
      #    - 'current_by_interval': Use current GTFS fleet by interval
      #    - 'current_peak': Use current peak fleet for all intervals
      #    - 'manual': Use manually specified values
      baseline: current_by_interval
      baseline_values: []  # Only used if baseline: manual. List of baseline fleet values per interval. Same length as intervals
      # tolerance: increase allowed per interval
      tolerance: 0.35 # If 0.1, then 10% increase allowed per interval
      # min_fraction: Allowed reduction below baseline per interval
      min_fraction: 0.8  # If 0.7, then maintain at least 70% of current service per interval

  # specific penalties for violating each constraint. Applies when optimization["algorithm"]["use_penalty_method"]: true
  penalty_weights:
    fleet_total: 1000           # FleetTotalConstraintHandler
    minimum_fleet: 1000         # MinimumFleetConstraintHandler
    fleet_per_interval: 1000    # FleetPerIntervalConstraintHandler

optimization:
  algorithm:
    # See pymoo documentation: https://pymoo.org/algorithms/soo/pso.html
    type: PSO
    pop_size: 50  # number of particales to use
    inertia_weight: 0.9
    cognitive_coeff: 2.0
    social_coeff: 2.0
    adaptive: true # whether to use adaptive PSO parameters (default pymoo implementation)
    # Penalty method is a custom implementation that guides the search, as PSO is not meant for constrained optimization
    # When penalty method is used:
    # - All solutions are considered "feasible" (violations in objective)
    # - Violation counts are tracked for analysis but don't filter solutions
    # - Best solutions ranked purely by objective value (which includes penalties)
    use_penalty_method: true
    # penalty_weight value should be relative to objective value (i.e if objective values around 1000, then set accordingly).
    # A penalty_weight that is very small relative to objective values will have no effect
    # Base penalty (adaptive if adaptive_penalty: true)
    penalty_weight: 500
    # whether to increase penalty weights incrementally during optimization
    # NOTE: Adaptive penalty only affects constraints that DO NOT have specific weights defined in problem["penalty_weights"]
    adaptive_penalty: false
    penalty_increase_rate: 1.1 # factor by which to increase penalty weights if adaptive_penalty: true
    # current_penalty = initial_penalty × (increase_rate ^ progress)
    # where progress = current_generation / max_generations (ranges from 0 to 1)
  sampling:
    enabled: true
    # Options are 'from_data' or a List of solutions
    base_solutions: 'from_data'
    # base_solutions:
    #    gtfs_paths:
    #        - "output/combined_solution_01_gtfs.zip"
    #        - "output/combined_solution_02_gtfs.zip"
    #    drt_solution_paths:
    #        - "output/combined_solution_01_drt.json"
    #        - "output/combined_solution_02_drt.json"
    # Our initial population = pop_size. If sampling, we use base_solutions, and fill the rest (pop_size  base_solutions)
    frac_gaussian_pert: 0.1 # fraction of ramining solutions to fill using gaussian noise around base_solutions). Rest will be filled with LHS
    gaussian_sigma: 1.5 # If sigma = 1.5, then 66% of values will be within 1.5 of original value. IMPORTANT: we use index values, so a change of 1 means jumping to the next headway / DRT fleet size
    #random_seed: 50 # Do not specify if using optimize_multi_swarm() i.e. if optimization["run"]["multi_swarm"]: true

  run:
    multi_swarm: true # if true, run multiple swarms in parallel
    num_runs: 5 # How many swarms to run. Only used if multi_swarm: true
    parallel: true # Run swarms in parallel or sequential. Only used if multi_swarm: true
    track_best_n: 10  # Track the best n results

  termination:
    max_generations: 50   # Total number of particles = pop_size x max_generations (x num_runs if multi_swarm: true)
    max_time_minutes: 60 # Stop after this time even if we haven't completed all generations

  monitoring:
    progress_frequency: 5
    save_history: false    # Whether to save Pymoo PSO optimization history (all solutions per generation)


output:
  # Where to write optimization outputs and exports (mirrors 3_write_solutions_to_file.ipynb)
  save_results: true
  results_dir: "output/test_run_py"     # base directory for all exports
  solution_prefix: "combined_solution"          # prefix to be used for all files
  best_run: false      # Whether to export only the best run's solutions (true) or all runs (false)
  max_to_save: 20      # Maximum number of solutions to export. Default: None (all)
  save_config: true   # Save the config in the output directory

logging:
  console_level: "INFO"
  file_level: "DEBUG"
  log_file: "transit_opt_run_2.log"    # log file name
  log_dir: "output/test_run_py"        # where to store log file (should be same dir/ as data output)
